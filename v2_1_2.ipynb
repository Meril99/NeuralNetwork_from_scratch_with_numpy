{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fb275c-be24-44e6-a767-f14fa00c444a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from sklearn.datasets import make_blobs\n",
    "from tqdm import tqdm #barre de progression taquadoum\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#sav files\n",
    "file1='t10k-images.idx3-ubyte'\n",
    "file2='t10k-labels.idx1-ubyte'\n",
    "file3='train-images.idx3-ubyte'\n",
    "file4='train-labels.idx1-ubyte'\n",
    "\n",
    "def read_idx(filename):\n",
    "    '''Reads an idx file and returns an ndarray'''\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "# def preprocessing(arr3d_array):\n",
    "#     arr2d_array=arr3d_array.reshape(arr3d_array.shape[0],arr3d_array.shape[1]*arr3d_array.shape[2])\n",
    "#     arr2d_array = normalize(arr2d_array)\n",
    "#     return arr2d_array\n",
    "  \n",
    "def preprocessing(arr3d_array):\n",
    "    arr2d_array=arr3d_array.reshape(arr3d_array.shape[0],arr3d_array.shape[1]*arr3d_array.shape[2])\n",
    "    arr2d_array = (arr2d_array - arr2d_array.min())/ (arr2d_array.max() - arr2d_array.min())\n",
    "    return arr2d_array\n",
    "    \n",
    "\n",
    "def preprocessing_label(vector):\n",
    "    matrice = np.zeros((vector.size, 10))\n",
    "    matrice[np.arange(vector.size), vector] = 1\n",
    "    return matrice\n",
    "    \n",
    "    \n",
    "\n",
    "def graphic_view():\n",
    "    X, y = make_blobs(n_samples=60000, n_features=28*28, centers=10, random_state=0)\n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "    print(X.shape,y.shape)\n",
    "    print('dimensions de X:', X.shape)\n",
    "    print('dimensions de y:', y.shape)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:, 1], c=y, cmap='summer')\n",
    "    plt.show()\n",
    "\n",
    "def weights_matrice(m,n):\n",
    "    matrix=np.random.standard_normal((m,n))\n",
    "    b = np.random.randn(1)\n",
    "    sqrt=math.sqrt(n)\n",
    "    scalaire=1/sqrt\n",
    "    matrix=scalaire*matrix\n",
    "    return matrix\n",
    "    \n",
    "    \n",
    "\n",
    "def sigmoide(z_i):\n",
    "    return 1/(1+np.exp(-z_i))\n",
    "\n",
    "# normaliser poids\n",
    "def softmax(array):\n",
    "    return (np.exp(array - array.max()))/np.sum(np.exp(array - array.max()))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass_batch(batch,w1_all__,w2_all__,w3_all__):\n",
    "    z1_all_images= batch.dot(w1_all__) # pour une division en 12000 batchs de x_train et 2000 de x_test: (5, 784) * 784x128 -> 5x128\n",
    "    a1_all_images= sigmoide(z1_all_images) #shape de z1 : 5x128 -> shape de a1_all_images: 5x128 \n",
    "    # On fait paser les sorties à la seconde couche de neurones\n",
    "    z2_all_images=a1_all_images.dot(w2_all__) #5x128 * 128x64 -> 5x64\n",
    "    a2_all_images=sigmoide(z2_all_images) # 5x64\n",
    "    # On fait paser les sorties à la troisième couche de neurones\n",
    "    z3_all_images=a2_all_images.dot(w3_all__) # -> z3 : 5x10\n",
    "    a3_all_images=softmax(z3_all_images) # -> a3_all : 5x10\n",
    "    return batch, z1_all_images, z2_all_images, z3_all_images, a1_all_images, a2_all_images, a3_all_images\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# un batch  après l'autre\n",
    "def backpropagation_batch(batch_,z1_all_images_,z2_all_images_,z3_all_images_,a1_all_images_,a2_all_images_,a3_all_images_,label_,w1__,w2__,w3__):\n",
    "    e_3_=a3_all_images_-label_ # toujours pour une division en 12 000 batchs de x_train et 2000 de x_test e3 : (5,10)\n",
    "    e_2_=np.multiply(e_3_.dot(w3__.T),dsigmoid(z2_all_images_)) # shape e2 : (5,64)\n",
    "    e_1_=np.multiply(e_2_.dot(w2__.T),dsigmoid(z1_all_images_)) # shape e1 : (5,128)\n",
    "    delta_w3_all=a2_all_images_.T.dot(e_3_) # shape :(64,10)\n",
    "    delta_w2_all=a1_all_images_.T.dot(e_2_) # shape :(128,64)\n",
    "    delta_w1_all=batch_.T.dot(e_1_) # shape :784,5 *5,128-> 784,128\n",
    "    return delta_w3_all, delta_w2_all, delta_w1_all\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def update(dw1, dw2,dw3,w_1__,w_2__,w_3__, lambd):\n",
    "    w_1_new = w_1__ - (lambd*dw1)\n",
    "    w_2_new = w_2__ - (lambd*dw2)\n",
    "    w_3_new = w_3__ - (lambd*dw3)\n",
    "    return (w_1_new,w_2_new,w_3_new)\n",
    "\n",
    "\n",
    "\n",
    "def compute_error(A,y):\n",
    "    epsilon = 1e-15\n",
    "    return 1 / len(y) * np.sum(-y * np.log(A + epsilon) - (1 - y) * np.log(1 - A + epsilon))\n",
    "\n",
    "def compute_batch_error(test_data_x_test,test_data_label_test,updated_w1,updated_w2,updated_w3):\n",
    "    error_total=np.empty([0,test_data_x_test.shape[0]],dtype=float)\n",
    "    for j in range(0,test_data_x_test.shape[0]):\n",
    "        new_test_data,new_z1__,new_z2__,new_z3__,new_a1__,new_a2__,new_a3__=forward_pass_batch(test_data_x_test[j],updated_w1,updated_w2,updated_w3)\n",
    "\n",
    "        indice_max_y=np.argmax(test_data_label_test[j],axis=1)\n",
    "       \n",
    "        indice_max_a=np.argmax(new_a3__,axis=1)\n",
    "       \n",
    "        error_per_batch=1-np.mean(indice_max_a==indice_max_y)\n",
    "        error_total=np.append(error_total,error_per_batch)\n",
    "        \n",
    "    return np.mean(error_total)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def intialisation(m,n,p,c):\n",
    "    poids1=weights_matrice(m,n) # W1 contient autant de paramètres que ce qu'il y de variables dans X_train\n",
    "    poids2=weights_matrice(n,p)\n",
    "    poids3=weights_matrice(p,c)\n",
    "    return (poids1,poids2,poids3)\n",
    "\n",
    "\n",
    "\n",
    "# toutes les images d'un seul coup en les découpant en n batchs\n",
    "def train_sets_batch(test_data_x,test_data_y,train_data_x, train_label_array, learning_rate = 0.155555555555, epochs = 100,batch=60):\n",
    "    \n",
    "    train_data_x=preprocessing(train_data_x)\n",
    "    train_label_array=preprocessing_label(train_label_array)\n",
    "    test_data_x=preprocessing(test_data_x)\n",
    "    test_data_y=preprocessing_label(test_data_y)\n",
    "    \n",
    "    sets_list=np.array_split(train_data_x, batch)\n",
    "    sets_array=np.array(sets_list)\n",
    "    y_sets=np.array_split(train_label_array, batch)\n",
    "    y_sets_array=np.array(y_sets)\n",
    "    batch_array=sets_array\n",
    "    \n",
    "    x_tests=np.array_split(test_data_x, batch/6)\n",
    "    x_tests=np.array(x_tests)\n",
    "    \n",
    "    y_tests=np.array_split(test_data_y, batch/6)\n",
    "    y_tests=np.array(y_tests)\n",
    "    \n",
    "\n",
    "    #entrainer batchs[0]  batchs[1], etc...\n",
    "    w1,w2,w3=intialisation(784,128,64,10)\n",
    "    for i in range(0,epochs):\n",
    "        for j in range(0,batch):\n",
    "            new_batch,z1__batch,z2__batch,z3__batch,a1__batch,a2__batch,a3__batch=forward_pass_batch(batch_array[j],w1,w2,w3)\n",
    "            delta_w3__,delta_w2__,delta_w1__=backpropagation_batch(new_batch,z1__batch,z2__batch,z3__batch,a1__batch,a2__batch,a3__batch,y_sets_array[j],w1,w2,w3)\n",
    "            w1_updated__,w2_updated__,w3_updated__=update(delta_w1__, delta_w2__,delta_w3__,w1,w2,w3, learning_rate)\n",
    "            w1,w2,w3=w1_updated__,w2_updated__,w3_updated__\n",
    "        mean_total_errors=compute_batch_error(x_tests,y_tests,w1_updated__,w2_updated__,w3_updated__)\n",
    "        print('epoch: ' + str(i) + ' error rate: ' + str(mean_total_errors)+'\\n')\n",
    "    error=mean_total_errors\n",
    "    return error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    x_train,y_train,x_test,y_test=read_idx(file3),read_idx(file4),read_idx(file1),read_idx(file2)\n",
    "\n",
    "\n",
    "\n",
    "    train_sets_batch(x_test,y_test,x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6f7a61d5-4c99-41f1-bf9c-001ab5a66dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_tests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_339452/3742539240.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_test shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_tests[0:6].shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tests\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_tests' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('y_test shape')\n",
    "print(y_tests.shape)\n",
    "print('y_tests[0:6].shape')\n",
    "print(y_tests[0:6].shape)\n",
    "print(y_tests[0:6])\n",
    "print('y_test[0] shape')\n",
    "print(y_tests[0].shape)\n",
    "print(y_tests[0])\n",
    "\n",
    "print('y_sets_array shape')\n",
    "print(y_sets_array.shape)\n",
    "print('y_sets_array[0:6].shape')\n",
    "print(y_sets_array[0:6].shape)\n",
    "print(y_sets_array[0:6])\n",
    "print('y_sets_array[0] shape')\n",
    "print(y_sets_array[0].shape)\n",
    "print(y_sets_array[0])\n",
    "\n",
    "\n",
    "print('x_test shape')\n",
    "print(x_tests.shape)\n",
    "print('x_tests[0:6].shape')\n",
    "print(x_tests[0:6].shape)\n",
    "print(x_tests[0:6])\n",
    "print('x_test[0] shape')\n",
    "print(x_tests[0].shape)\n",
    "print(x_tests[0])\n",
    "\n",
    "\n",
    "print('batch_array shape')\n",
    "print(batch_array.shape)\n",
    "print('batch_array[0:6].shape')\n",
    "print(batch_array[0:6].shape)\n",
    "print(batch_array[0:6])\n",
    "print('batch_array[0] shape')\n",
    "print(batch_array[0].shape)\n",
    "print(batch_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04007a-bc5d-4a7b-8af2-caf6ceb9ae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018c9e7-dae9-4e9c-86c0-750670625969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2e18c-90b3-408e-9016-fb157849da6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3063f-b54f-4766-af07-53ffb1366382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918e810-ee73-476a-9c6b-d7c43a5ed349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3b111-9170-4f0d-a44b-5ac0e7bdf0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65faa3c-d545-4008-abc7-536d4b46c128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455881e5-48c7-4dcc-b712-8ba980e5a722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
