{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76e5da1-54a9-46fa-85a0-4052b1ae2df3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "from sklearn.datasets import make_blobs\n",
    "from tqdm import tqdm #barre de progression taquadoum\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7797bb06-fdfc-40a8-a7a9-5cadb7aeca32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file1='t10k-images.idx3-ubyte'\n",
    "file2='t10k-labels.idx1-ubyte'\n",
    "file3='train-images.idx3-ubyte'\n",
    "file4='train-labels.idx1-ubyte'\n",
    "\n",
    "def read_idx(filename):\n",
    "    '''Reads an idx file and returns an ndarray'''\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8f1cb8-9be8-4ba6-8b2b-c1d63821e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(arr3d_array):\n",
    "#     arr2d_array=arr3d_array.reshape(arr3d_array.shape[0],arr3d_array.shape[1]*arr3d_array.shape[2])\n",
    "#     arr2d_array = normalize(arr2d_array)\n",
    "#     return arr2d_array\n",
    "  \n",
    "def preprocessing(arr3d_array):\n",
    "    arr2d_array=arr3d_array.reshape(arr3d_array.shape[0],arr3d_array.shape[1]*arr3d_array.shape[2])\n",
    "    arr2d_array = (arr2d_array - arr2d_array.min())/ (arr2d_array.max() - arr2d_array.min())\n",
    "    return arr2d_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd30cf7-52f1-4835-8727-c2380a6fee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_label(vector):\n",
    "    matrice = np.zeros((vector.size, 10))\n",
    "    matrice[np.arange(vector.size), vector] = 1\n",
    "    return matrice\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376f71c-2ec8-41b4-8ba8-b12230e5763d",
   "metadata": {},
   "source": [
    "### Apperçu dans un graphe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62ee977-c10b-4aa0-9a49-982d6c5babf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_blobs(n_samples=60000, n_features=28*28, centers=10, random_state=0)\n",
    "# y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "# print(X.shape,y.shape)\n",
    "# print('dimensions de X:', X.shape)\n",
    "# print('dimensions de y:', y.shape)\n",
    "\n",
    "# plt.scatter(X[:,0], X[:, 1], c=y, cmap='summer')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf1be8-f46c-4a4e-a899-4dfca15bb962",
   "metadata": {},
   "source": [
    "# Defining FFNN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2baa83cc-2fde-4945-9c4a-dae2009b7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_matrice(m,n):\n",
    "    matrix=np.random.standard_normal((m,n))\n",
    "    sqrt=math.sqrt(n)\n",
    "    scalaire=1/sqrt\n",
    "    matrix=scalaire*matrix\n",
    "    return matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2469084-08fc-4a79-88c4-7847821a845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z_i):\n",
    "    return 1/(1+np.exp(-z_i))\n",
    "\n",
    "# normaliser poids\n",
    "def softmax(array):\n",
    "    return (np.exp(array - array.max()))/np.sum(np.exp(array - array.max()))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "# # derivative of sigmoid\n",
    "# # sigmoid(y) * (1.0 - sigmoid(y))\n",
    "# # the way we use this y is already sigmoided\n",
    "# def dsigmoid(x):\n",
    "#     return np.multiply(sigmoide(x),(1.0 - sigmoide(x)))\n",
    "\n",
    "# pour envoyer d'un seul cooup tout un set d'images ou toutes les images d'un coup\n",
    "def forward_pass_all_1(batches_of_data,w1_all__,w2_all__,w3_all__):\n",
    "    #Première couche\n",
    "    z1_all_images= np.tensordot(batches_of_data, w1_all__,axes=((2),(0))) #(1,60000, 784) * 784x128 -> 12000x5x128\n",
    "    a1_all_images= sigmoide(z1_all_images) #shape de z1 : 1,60000,128 -> shape de a1_all_images:  1,60000,128 \n",
    "    # On fait paser les sorties à la seconde couche de neurones\n",
    "    z2_all_images=np.tensordot(a1_all_images,w2_all__, axes=((2),(0)))  #1,60000,128  * 128x64 -> 1,60000,64\n",
    "    a2_all_images=sigmoide(z2_all_images) # 1,60000,128\n",
    "    # On fait paser les sorties à la troisième couche de neurones\n",
    "    z3_all_images=np.tensordot(a2_all_images,w3_all__, axes=((2),(0))) # -> z3 : 1,60000,10\n",
    "    a3_all_images=softmax(z3_all_images) # -> a3_all : 1,60000,10\n",
    "    return (batches_of_data,z1_all_images,z2_all_images,z3_all_images,a1_all_images,a2_all_images,a3_all_images)\n",
    "\n",
    "def forward_pass_all_2(matrix,w1_all,w2_all,w3_all):\n",
    "    #shape de matrix : (60000,784)\n",
    "    # 1. aggrégation\n",
    "    z1_all_images= matrix.dot(w1_all) #  60000x786 dot  786x128 -> 60000x128\n",
    "    # 2. activation\n",
    "    a1_all_images= sigmoide(z1_all_images) #shape de z1 : 60000x128\n",
    "    # On fait paser les sorties à la seconde couche de neurones\n",
    "    z2_all_images=a1_all_images.dot(w2_all) #60000x128 * 128x64 -> 60000x64\n",
    "    a2_all_images=sigmoide(z2_all_images) # 60000x64\n",
    "    z3_all_images=a2_all_images.dot(w3_all) # -> z3 : 60000x10\n",
    "    a3_all_images=softmax(z3_all_images) # -> a3 : 60000x10\n",
    "    return (matrix,z1_all_images,z2_all_images,z3_all_images,a1_all_images,a2_all_images,a3_all_images)\n",
    "\n",
    "\n",
    "# pour toutes les images d'un seul coup\n",
    "def backpropagation_all_1(batches_, z_1_all, z_2_all, z3, a_3_all, a_2_all, a_1_all, label, w1_, w2_, w3_):\n",
    "    e_3 = a_3_all - label\n",
    "    e_2 = np.multiply(e_3.dot(w3_.T), dsigmoid(z_2_all))  # shape e2 : (1,60000,64)\n",
    "    e_1 = np.multiply(e_2.dot(w2_.T), dsigmoid(z_1_all))  # shape e1 : (1,60000,128)\n",
    "    delta_w3_all = np.tensordot(a_2_all.T, e_3, axes=([2,1], [0,1]))  # shape :(128,60000,1)*(1,60000,10) -> (128,10)\n",
    "    delta_w2_all = np.tensordot(a_1_all.T, e_2,  axes=([2,1], [0,1]))  # shape :(128,64)\n",
    "    delta_w1_all = np.tensordot(batches_.T, e_1,  axes=([2,1], [0,1]))  # shape :(784,128)\n",
    "    return (delta_w3_all, delta_w2_all, delta_w1_all)\n",
    "\n",
    "\n",
    "def backpropagation_all_2(matrix_, z_1_all, z_2_all, z3, a_3_all, a_2_all, a_1_all, label, w1_, w2_, w3_):\n",
    "    e_3 = a_3_all - label  # e3 : (60000,10)\n",
    "    e_2 = np.multiply(e_3.dot(w3_.T), dsigmoid(z_2_all))  # shape e2 : (60000, 64)\n",
    "    e_1 = np.multiply(e_2.dot(w2_.T), dsigmoid(z_1_all))  # shape e1 : (60000, 128)\n",
    "\n",
    "    delta_w3_all = a_2_all.T.dot(e_3)  # shape :(64,10)\n",
    "    delta_w2_all = a_1_all.T.dot(e_2)  # shape : (128, 64)\n",
    "    delta_w1_all = matrix_.T.dot(e_1)  # shape: (784, 128)\n",
    "\n",
    "    return (delta_w3_all, delta_w2_all, delta_w1_all)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def update(dw1, dw2,dw3,w_1__,w_2__,w_3__, lambd):\n",
    "    w_1_new = w_1__ - (lambd*dw1)\n",
    "    w_2_new = w_2__ - (lambd*dw2)\n",
    "    w_3_new = w_3__ - (lambd*dw3)\n",
    "    \n",
    "   \n",
    "\n",
    "    return (w_1_new,w_2_new,w_3_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e77bc7-2a45-49dc-b4be-0db26e230405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(A,y):\n",
    "    epsilon = 1e-15\n",
    "    return 1 / len(y) * np.sum(-y * np.log(A + epsilon) - (1 - y) * np.log(1 - A + epsilon))\n",
    "\n",
    "def compute_error_all_1(test_data_x_test,test_data_label_test,updated_w1,updated_w2,updated_w3):\n",
    "    # error_total=np.empty([0,test_data_x_test.shape[0]],dtype=float)\n",
    "    # for j in range(0,test_data_x_test.shape[0]):\n",
    "    new_test_data,new_z1__,new_z2__,new_z3__,new_a1__,new_a2__,new_a3__=forward_pass_all_1(test_data_x_test,updated_w1,updated_w2,updated_w3)\n",
    "        # print('test_data_label, from batch ['+str(j)+'] without reshape')\n",
    "        # print(test_data_label_test[j])\n",
    "        # print(test_data_label_test[j].shape)\n",
    "        # print('indice y')\n",
    "        # indice_max_y=np.argmax(y_batches[j],axis=1).reshape((y_batches[j].shape[0],1))\n",
    "    indice_max_y=np.argmax(test_data_label_test,axis=2)\n",
    "        # print(indice_max_y)\n",
    "        # print(indice_max_y.shape)\n",
    "        # print('new_a3__ from batch['+str(j)+']')\n",
    "        # print(new_a3__)\n",
    "        # print(new_a3__.shape)\n",
    "        # print('indice a')\n",
    "    indice_max_a=np.argmax(new_a3__,axis=2)\n",
    "        # indice_max_a=np.argmax(a3__,axis=1).reshape((a3__.shape[0],1))\n",
    "        # print(indice_max_a)\n",
    "        # print(indice_max_a.shape)\n",
    "    error_per_batch=1-np.mean(indice_max_a==indice_max_y)\n",
    "    # error_total=np.append(error_total,error_per_batch)\n",
    "        # print('error_total')\n",
    "        # print(np.mean(error_total))\n",
    "    return error_per_batch\n",
    "\n",
    "def compute_error_all_2(test_data_x_test,test_data_label_test,updated_w1,updated_w2,updated_w3):\n",
    "    # error_total=np.empty([0,test_data_x_test.shape[0]],dtype=float)\n",
    "    # for j in range(0,test_data_x_test.shape[0]):\n",
    "    new_test_data,new_z1__,new_z2__,new_z3__,new_a1__,new_a2__,new_a3__=forward_pass_all_2(test_data_x_test,updated_w1,updated_w2,updated_w3)\n",
    "        # print('test_data_label, from batch ['+str(j)+'] without reshape')\n",
    "        # print(test_data_label_test[j])\n",
    "        # print(test_data_label_test[j].shape)\n",
    "        # print('indice y')\n",
    "        # indice_max_y=np.argmax(y_batches[j],axis=1).reshape((y_batches[j].shape[0],1))\n",
    "    indice_max_y=np.argmax(test_data_label_test,axis=1)\n",
    "        # print(indice_max_y)\n",
    "        # print(indice_max_y.shape)\n",
    "        # print('new_a3__ from batch['+str(j)+']')\n",
    "        # print(new_a3__)\n",
    "        # print(new_a3__.shape)\n",
    "        # print('indice a')\n",
    "    indice_max_a=np.argmax(new_a3__,axis=1)\n",
    "        # indice_max_a=np.argmax(a3__,axis=1).reshape((a3__.shape[0],1))\n",
    "        # print(indice_max_a)\n",
    "        # print(indice_max_a.shape)\n",
    "    error_per_batch=1-np.mean(indice_max_a==indice_max_y)\n",
    "    # error_total=np.append(error_total,error_per_batch)\n",
    "        # print('error_total')\n",
    "        # print(np.mean(error_total))\n",
    "    return error_per_batch\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae31308e-9732-4b17-9915-c8e3508e90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialisation(m,n,p,c):\n",
    "    poids1=weights_matrice(m,n) # W1 contient autant de paramètres que ce qu'il y de variables dans X_train\n",
    "    poids2=weights_matrice(n,p)\n",
    "    poids3=weights_matrice(p,c)\n",
    "    return (poids1,poids2,poids3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55964e68-7100-4c1d-a6fd-5759184a564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# toutes les images d'un seul coup en les découpant en 1 sel batch\n",
    "def train_all_1(test_data_x,test_data_y,train_data_x, train_label_array, learning_rate = 100, epochs = 30):\n",
    "    train_data_x=preprocessing(train_data_x)\n",
    "    train_label_array=preprocessing_label(train_label_array)\n",
    "    test_data_x=preprocessing(test_data_x)\n",
    "    test_data_y=preprocessing_label(test_data_y)\n",
    "    train_data_x=np.expand_dims(train_data_x, axis=0)\n",
    "    train_label_array=np.expand_dims(train_label_array, axis=0)\n",
    "    test_data_x=np.expand_dims(test_data_x, axis=0)\n",
    "    test_data_y=np.expand_dims(test_data_y, axis=0)    \n",
    "    w1,w2,w3=intialisation(784,128,64,10)\n",
    "    for i in range(0,epochs):\n",
    "        new_batch,z1__batch,z2__batch,z3__batch,a1__batch,a2__batch,a3__batch=forward_pass_all_1(train_data_x,w1,w2,w3)\n",
    "        delta_w3__,delta_w2__,delta_w1__=backpropagation_all_1(new_batch,z1__batch,z2__batch,z3__batch,a3__batch,a2__batch,a1__batch,train_label_array,w1,w2,w3)\n",
    "        w1_updated__,w2_updated__,w3_updated__=update(delta_w1__, delta_w2__,delta_w3__,w1,w2,w3, learning_rate)\n",
    "        w1,w2,w3=w1_updated__,w2_updated__,w3_updated__\n",
    "        mean_total_errors=compute_error_all_1(test_data_x,test_data_y,w1_updated__,w2_updated__,w3_updated__)\n",
    "        print('epoch: ' + str(i) + ' error rate: ' + str(mean_total_errors)+'\\n')\n",
    "    return mean_total_errors\n",
    "\n",
    "\n",
    "def train_all_2(test_data_x,test_data_y,train_data_x, train_label_array, learning_rate = 100, epochs = 30):\n",
    "    train_data_x=preprocessing(train_data_x)\n",
    "    train_label_array=preprocessing_label(train_label_array)\n",
    "    test_data_x=preprocessing(test_data_x)\n",
    "    test_data_y=preprocessing_label(test_data_y)\n",
    "    w1,w2,w3=intialisation(784,128,64,10)\n",
    "    for i in range(0,epochs):\n",
    "        new_batch,z1__batch,z2__batch,z3__batch,a1__batch,a2__batch,a3__batch=forward_pass_all_1(train_data_x,w1,w2,w3)\n",
    "        delta_w3__,delta_w2__,delta_w1__=backpropagation_all_1(new_batch,z1__batch,z2__batch,z3__batch,a3__batch,a2__batch,a1__batch,train_label_array,w1,w2,w3)\n",
    "        w1_updated__,w2_updated__,w3_updated__=update(delta_w1__, delta_w2__,delta_w3__,w1,w2,w3, learning_rate)\n",
    "        w1,w2,w3=w1_updated__,w2_updated__,w3_updated__\n",
    "        mean_total_errors=compute_error_all_1(test_data_x,test_data_y,w1_updated__,w2_updated__,w3_updated__)\n",
    "        print('epoch: ' + str(i) + ' error rate: ' + str(mean_total_errors)+'\\n')\n",
    "    return mean_total_errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7af5d3c-7b11-49c7-9fb1-feaf25043014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test=read_idx(file3),read_idx(file4),read_idx(file1),read_idx(file2)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "436c2721-201f-46c0-bbd1-e51352828c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 error rate: 0.8876333333333333\n",
      "\n",
      "epoch: 1 error rate: 0.9012833333333333\n",
      "\n",
      "epoch: 2 error rate: 0.9012833333333333\n",
      "\n",
      "epoch: 3 error rate: 0.9012833333333333\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49481/3652307852.py\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#efficient result on five images if you cut into 2 batches and a learing rate of 0.01 and less than 30 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_all_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_49481/1906410932.py\u001b[0m in \u001b[0;36mtrain_all_1\u001b[0;34m(test_data_x, test_data_y, train_data_x, train_label_array, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintialisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mnew_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz1__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz2__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz3__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma3__batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward_pass_all_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdelta_w3__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_w2__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_w1__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackpropagation_all_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz1__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz2__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz3__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma3__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1__batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mw1_updated__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2_updated__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3_updated__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_w1__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_w2__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_w3__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49481/3092112538.py\u001b[0m in \u001b[0;36mforward_pass_all_1\u001b[0;34m(batches_of_data, w1_all__, w2_all__, w3_all__)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#Première couche\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mz1_all_images\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_of_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1_all__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(1,60000, 784) * 784x128 -> 12000x5x128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0ma1_all_images\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msigmoide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1_all_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#shape de z1 : 1,60000,128 -> shape de a1_all_images:  1,60000,128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# On fait paser les sorties à la seconde couche de neurones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mz2_all_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1_all_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2_all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#1,60000,128  * 128x64 -> 1,60000,64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49481/3092112538.py\u001b[0m in \u001b[0;36msigmoide\u001b[0;34m(z_i)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# normaliser poids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train(x_train_copy, y_train_copy)\n",
    "# you get better results if you pass in fewer images and use bigger epochs on them\n",
    "\n",
    "#you get better results if you train on only a set of 5 images, learning rate of 0.01 and epochs = 60\n",
    "# train_all_images(x_train_copy[0:5], y_train_copy[0:5])\n",
    "\n",
    "\n",
    "#efficient result on five images if you cut into 2 batches and a learing rate of 0.01 and less than 30 epochs\n",
    "# train_sets(x_train_copy[0:5], y_train_copy[0:5])\n",
    "\n",
    "#efficient result on five images if you cut into 2 batches and a learing rate of 0.01 and less than 30 epochs\n",
    "train_all_1(x_train,y_train,x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a61d5-4c99-41f1-bf9c-001ab5a66dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04007a-bc5d-4a7b-8af2-caf6ceb9ae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018c9e7-dae9-4e9c-86c0-750670625969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2e18c-90b3-408e-9016-fb157849da6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3063f-b54f-4766-af07-53ffb1366382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918e810-ee73-476a-9c6b-d7c43a5ed349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3b111-9170-4f0d-a44b-5ac0e7bdf0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65faa3c-d545-4008-abc7-536d4b46c128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455881e5-48c7-4dcc-b712-8ba980e5a722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
